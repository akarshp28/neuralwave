{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2GEa2q7o9y6S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(time):\n",
    "    if time < 60:\n",
    "        string = \"{:.0f}s\".format(time)\n",
    "    elif time < 3600:\n",
    "        string = \"{:.0f}m\".format(time/60)\n",
    "    else:\n",
    "        string = \"{:.2f}h\".format(time/60/60)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L101\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = [g for g, _ in grad_and_vars]\n",
    "        grad = tf.reduce_mean(grads, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0dluPE86w7qB"
   },
   "outputs": [],
   "source": [
    "#Seq2Seq Autoencoder with a single LSTM for both encoder and decoder,\n",
    "#along with optional rollout for the decoder LSTM\n",
    "def autoencoder(i, inputs, rollout):\n",
    "    \n",
    "    def encoder_loop_fn(time, cell_output, cell_state, loop_state):\n",
    "        emit_output = cell_output  # == None for time == 0\n",
    "        \n",
    "        if cell_output is None:  # time == 0\n",
    "            next_cell_state = encoder_cell.zero_state(batch_size, tf.float32)\n",
    "        else:\n",
    "            next_cell_state = cell_state\n",
    "            \n",
    "        elements_finished = (time >= sequence_length)\n",
    "        finished = tf.reduce_all(elements_finished)\n",
    "        \n",
    "        next_input = tf.cond(finished,\n",
    "                             lambda: tf.zeros([batch_size, input_width], dtype=tf.float32),\n",
    "                             lambda: inputs_ta.read(time))\n",
    "        \n",
    "        next_loop_state = None\n",
    "        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "\n",
    "    def decoder_loop_fn(time, cell_output, cell_state, loop_state):         \n",
    "        emit_output = cell_output\n",
    "        \n",
    "        if cell_output is None:  # time == 0\n",
    "            next_cell_state = encoder_cell_states\n",
    "            next_input = tf.zeros([batch_size, input_width], dtype=tf.float32)\n",
    "        else:\n",
    "            next_cell_state = cell_state\n",
    "            next_input = tf.cond(rollout,\n",
    "                                 lambda: tf.layers.dense(cell_output, input_width, activation=tf.nn.sigmoid, name=\"FC1\", reuse=tf.AUTO_REUSE), \n",
    "                                 lambda: inputs_ta.read(time-1))\n",
    "            \n",
    "        elements_finished = (time >= sequence_length)\n",
    "        \n",
    "        next_loop_state = None\n",
    "        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "    with tf.name_scope('autoencoder_{}'.format(i)):\n",
    "        #convert sample into tensor array\n",
    "        inputs_ta = tf.TensorArray(dtype=tf.float32, size=sequence_length, clear_after_read=False)\n",
    "        inputs_ta = inputs_ta.unstack(inputs)\n",
    "\n",
    "        with tf.name_scope('encoder'):\n",
    "            #encoder lstm\n",
    "            encoder_cell = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "            _, encoder_cell_states, _ = tf.nn.raw_rnn(encoder_cell, encoder_loop_fn)\n",
    "\n",
    "        with tf.name_scope('decoder'):\n",
    "            #decoder lstm\n",
    "            decoder_cell = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "            decoder_hidden_states_ta, _, _ = tf.nn.raw_rnn(decoder_cell, decoder_loop_fn)\n",
    "\n",
    "        #convert lstm output array into a tensor\n",
    "        outputs = decoder_hidden_states_ta.stack()\n",
    "        outputs = tf.layers.dense(outputs, input_width, activation=tf.nn.sigmoid, name=\"FC1\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    with tf.name_scope(\"loss_{}\".format(i)):       \n",
    "        loss = tf.reduce_mean(tf.square(inputs-outputs))\n",
    "    \n",
    "    return encoder_cell_states, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    features = {\"data\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "                \"label\": tf.FixedLenFeature((), tf.int64 , default_value=0)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    data = tf.decode_raw(parsed_features['data'], tf.float32)\n",
    "    data = tf.reshape(data, [8000, 540])\n",
    "    \n",
    "    data = data[:, :270]\n",
    "    data.set_shape((input_width, sequence_length))\n",
    "    \n",
    "    label = tf.cast(parsed_features['label'], tf.int32)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    filenames = tf.placeholder(tf.string, shape=[None])\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=num_classes)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    dataset = dataset.repeat(repeat)\n",
    "    dataset = dataset.shuffle(buffer_size=2000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(4 * batch_size * num_gpus)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    learning_rate = tf.train.exponential_decay(lr, \n",
    "                                               global_step, \n",
    "                                               train_steps, \n",
    "                                               decay_rate, \n",
    "                                               staircase=True) \n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    rollout = tf.constant(rollout_, dtype=tf.bool)\n",
    "    \n",
    "    tower_grads = []\n",
    "    losses = []\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "        for i in range(num_gpus):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('Tower_%d' % (i)) as scope:\n",
    "                    # Dequeues one batch for the GPU\n",
    "                    inputs, labels = iterator.get_next()\n",
    "                    inputs = tf.transpose(inputs, perm=[2, 0, 1])\n",
    "                    inputs.set_shape([sequence_length, batch_size, input_width])\n",
    "\n",
    "                    encoder_cell_states, loss = autoencoder(i, inputs, rollout)\n",
    "                    grads = opt.compute_gradients(loss)\n",
    "                    tower_grads.append(grads)\n",
    "                    losses.append(loss)\n",
    "\n",
    "    gradients = average_gradients(tower_grads)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    apply_gradient_op = opt.apply_gradients(gradients, global_step)\n",
    "    \n",
    "    avg_loss = tf.reduce_mean(losses)\n",
    "    tf.summary.scalar(\"avg_loss\", avg_loss) \n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "    merged = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    return init, merged, saver, global_step, avg_loss, apply_gradient_op, encoder_cell_states, labels, iterator, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train/Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 8m - loss: 0.01205\n",
      "Testing:\n",
      " - 6/6 - 57s - loss: 0.0112\n",
      "Epoch 2/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01192\n",
      "Testing:\n",
      " - 6/6 - 56s - loss: 0.0119\n",
      "Epoch 3/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01238\n",
      "Testing:\n",
      " - 6/6 - 56s - loss: 0.0114\n",
      "Epoch 4/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01161\n",
      "Testing:\n",
      " - 6/6 - 56s - loss: 0.0118\n",
      "Epoch 5/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01214\n",
      "Testing:\n",
      " - 6/6 - 56s - loss: 0.0119\n",
      "Epoch 6/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01141\n",
      "Testing:\n",
      " - 6/6 - 56s - loss: 0.0114\n",
      "Epoch 7/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01101\n",
      "Testing:\n",
      " - 6/6 - 56s - loss: 0.0108\n",
      "Epoch 8/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01144\n",
      "Testing:\n",
      " - 6/6 - 56s - loss: 0.0114\n",
      "Epoch 9/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01078\n",
      "Testing:\n",
      " - 6/6 - 56s - loss: 0.0112\n",
      "Epoch 10/10\n",
      "\n",
      "Training:\n",
      " - 34/34 - 7m - loss: 0.01064\n",
      "Testing:\n",
      " - 6/6 - 55s - loss: 0.0113\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/home/kalvik/shared/CSI_DATA/tfrecords/train/\"\n",
    "test_path = \"/home/kalvik/shared/CSI_DATA/tfrecords/test/\"\n",
    "\n",
    "train_filenames = [train_path+file for file in os.listdir(train_path)]\n",
    "test_filenames = [test_path+file for file in os.listdir(test_path)]\n",
    "\n",
    "weight_path = \"/home/kalvik/shared/neuralwave/autoencoder/weights/mse_amp_8000_4000/\"\n",
    "tensorboard_path = \"/home/kalvik/shared/neuralwave/autoencoder/tensorboard/train_mse_amp_8000_4000_\"\n",
    "sequence_length = 270\n",
    "input_width = 8000 \n",
    "decay_rate = 0.9\n",
    "lstm_size = 4000\n",
    "rollout_ = False\n",
    "batch_size = 8\n",
    "save_epoch = 5\n",
    "num_gpus = 4\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "repeat = -1\n",
    "train_samples = 1096\n",
    "test_samples = 194\n",
    "num_classes = len(train_filenames)\n",
    "train_steps = int(train_samples//(batch_size*num_gpus))\n",
    "test_steps = int(test_samples//(batch_size*num_gpus))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    init, merged, saver, global_step, avg_loss, apply_gradient_op, encoder_cell_states, labels, iterator, filenames = model()\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "        if tf.train.latest_checkpoint(weight_path) != None:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(weight_path))\n",
    "        else:\n",
    "            sess.run(init)\n",
    "            \n",
    "        train_writer = tf.summary.FileWriter(tensorboard_path+\"train\", sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(tensorboard_path+\"test\", sess.graph)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(\"\\nEpoch {}/{}\".format(epoch, epochs))\n",
    "            batch_time = []\n",
    "            epoch_time = time.time()\n",
    "    \n",
    "            #Training\n",
    "            print(\"\\nTraining:\")\n",
    "            sess.run(iterator.initializer, feed_dict={filenames: train_filenames})\n",
    "            for step in range(1, train_steps + 1):\n",
    "                time_start = time.time()\n",
    "                _, batch_loss, summary, g_step = sess.run([apply_gradient_op, avg_loss, merged, global_step])\n",
    "                batch_time.append(time.time()-time_start)\n",
    "                \n",
    "                train_writer.add_summary(summary)\n",
    "                \n",
    "                if (step == train_steps):\n",
    "                    sys.stdout.write(\"\\r - {}/{} - {} - loss: {:.4f}\".format(step, train_steps, get_time(time.time()-epoch_time), batch_loss))\n",
    "                else:\n",
    "                    sys.stdout.write(\"\\r - {}/{} - {} - loss: {:.4f}\".format(step, train_steps, get_time(np.mean(batch_time)*(train_steps-step)), batch_loss))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                \n",
    "            #Testing\n",
    "            batch_time = []\n",
    "            epoch_time = time.time()\n",
    "            print(\"\\nTesting:\")\n",
    "            sess.run(iterator.initializer, feed_dict={filenames: test_filenames})\n",
    "            for step in range(1, test_steps + 1):\n",
    "                time_start = time.time()\n",
    "                batch_loss, summary = sess.run([avg_loss, merged])\n",
    "                batch_time.append(time.time()-time_start)\n",
    "                \n",
    "                test_writer.add_summary(summary)\n",
    "                \n",
    "                if (step == test_steps):\n",
    "                    sys.stdout.write(\"\\r - {}/{} - {} - loss: {:.4f}\".format(step, test_steps, get_time(time.time()-epoch_time), batch_loss))\n",
    "                else:\n",
    "                    sys.stdout.write(\"\\r - {}/{} - {} - loss: {:.4f}\".format(step, test_steps, get_time(np.mean(batch_time)*(test_steps-step)), batch_loss))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            #save model\n",
    "            if epoch % save_epoch == 0:\n",
    "                saver.save(sess, os.path.join(weight_path, \"autoencoder_loss-{}_epoch-{}\".format(batch_loss, epoch)), global_step=g_step)\n",
    "\n",
    "        #save model\n",
    "        saver.save(sess, os.path.join(weight_path, \"autoencoder_loss-{}_epoch-{}\".format(batch_loss, epoch)), global_step=g_step)\n",
    "\n",
    "        print(\"\\nFinished!\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "autoencoder.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
