{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block, is_training):\n",
    "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters[0], kernel_size[0], padding=\"same\")\n",
    "\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters[1], kernel_size[1], padding=\"same\")\n",
    "\n",
    "        x = tf.concat([x, input_tensor], axis=-1)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, is_training, strides=2):\n",
    "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters[0], kernel_size[0], strides=strides, padding=\"same\")\n",
    "\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters[1], kernel_size[1], padding=\"same\")\n",
    "\n",
    "        shortcut = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
    "        shortcut = tf.nn.relu(shortcut)\n",
    "        shortcut = tf.layers.conv2d(shortcut, filters[1], kernel_size[1], strides=strides, padding=\"same\")\n",
    "\n",
    "        x = tf.concat([x, shortcut], axis=-1)\n",
    "    return x\n",
    "\n",
    "def classifier(inputs, num_classes, num_embeddings, is_training):\n",
    "    with tf.variable_scope(\"stage-1_block-a\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(inputs, 64, 7, strides=2, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.max_pooling2d(x, 3, strides=2, padding=\"same\")\n",
    "\n",
    "    x = conv_block    (x, [1, 3], [64, 256], stage=2, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [1, 3], [64, 256], stage=2, block='b', is_training=is_training)\n",
    "\n",
    "    x = conv_block    (x, [1, 3], [128, 512], stage=3, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [1, 3], [128, 512], stage=3, block='b', is_training=is_training)\n",
    "\n",
    "    x = conv_block    (x, [1, 3], [256, 1024], stage=4, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [1, 3], [256, 1024], stage=4, block='b', is_training=is_training)\n",
    "\n",
    "    x = conv_block    (x, [1, 3], [512, 2048], stage=5, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [1, 3], [512, 2048], stage=5, block='b', is_training=is_training)\n",
    "\n",
    "    with tf.variable_scope(\"embeddings\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.average_pooling2d(x, (x.get_shape()[-3], x.get_shape()[-2]), 1) #global average pooling\n",
    "        x = tf.layers.flatten(x)\n",
    "        embeddings = tf.layers.dense(x, num_embeddings, activation=None)\n",
    "        \n",
    "    with tf.variable_scope(\"logits\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        pred_logits = tf.nn.relu(embeddings)\n",
    "        pred_logits = tf.layers.dense(pred_logits, num_classes, activation=None)\n",
    "\n",
    "    return embeddings, pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_loss(features, labels, alpha, num_classes):\n",
    "    \n",
    "    len_features = features.get_shape()[1] \n",
    "    with tf.variable_scope(\"central_loss\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        centers = tf.get_variable('centers', [num_classes, len_features], dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "    labels = tf.reshape(labels, [-1])\n",
    "\n",
    "    centers_batch = tf.gather(centers, labels)\n",
    "    loss = tf.nn.l2_loss(features - centers_batch)\n",
    "\n",
    "    unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
    "    appear_times = tf.gather(unique_count, unique_idx)\n",
    "    appear_times = tf.reshape(appear_times, [-1, 1])\n",
    "\n",
    "    diff = centers_batch - features\n",
    "    diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
    "    diff = alpha * diff\n",
    "\n",
    "    centers_update_op = tf.scatter_sub(centers, labels, diff)\n",
    "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, centers_update_op)\n",
    "\n",
    "    return loss, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p], b[p]\n",
    "\n",
    "def get_batch(data_x, data_y, batch_size):\n",
    "    while True:\n",
    "        data_x, data_y = unison_shuffled_copies(data_x, data_y)\n",
    "        for index in range(0, data_x.shape[0], batch_size):\n",
    "            x, y = data_x[index:index+batch_size], data_y[index:index+batch_size]\n",
    "            if x.shape[0] == batch_size:\n",
    "                yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '/home/kjakkala/neuralwave/data/CSI_preprocessed_35.h5'\n",
    "intruder_dir=\"/home/kjakkala/neuralwave/data/CSI_preprocessed_Intruder.h5\"\n",
    "tensorboard_path = \"/home/kjakkala/neuralwave/data/logs/central_loss_135_99/\"\n",
    "weights_path =  \"/home/kjakkala/neuralwave/data/weights/central_loss_135_99/central_loss_model.ckpt\"\n",
    "num_embeddings = 64\n",
    "num_classes = 35\n",
    "save_step = 10\n",
    "decay_rate = 0.99\n",
    "batch_size = 32\n",
    "num_gpus = 4\n",
    "alpha = 0.5\n",
    "ratio = 0.5\n",
    "epochs = 200\n",
    "lr = 1e-4\n",
    "\n",
    "hf = h5py.File(data_file, 'r')\n",
    "X_train = np.expand_dims(hf.get('X_train'), axis=-1)[:, :, ::2]\n",
    "X_test = np.expand_dims(hf.get('X_test'), axis=-1)[:, :, ::2]\n",
    "y_train = np.eye(num_classes)[hf.get('y_train')]\n",
    "y_test = np.eye(num_classes)[hf.get('y_test')]\n",
    "\n",
    "y_train_n = np.array(hf.get('y_train'))\n",
    "y_test_n = np.array(hf.get('y_test'))\n",
    "\n",
    "hf.close()\n",
    "\n",
    "hf = h5py.File(intruder_dir, 'r')\n",
    "X_data = np.expand_dims(hf.get('X_data'), axis=-1)[:, :, ::2]\n",
    "y_data = np.ones(X_data.shape[0])*-1\n",
    "hf.close()\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_data.shape, y_data.shape, \"\\n\")\n",
    "\n",
    "train_steps = X_train.shape[0]//(batch_size*num_gpus)\n",
    "test_steps = X_test.shape[0]//(batch_size*num_gpus)\n",
    "train_data = get_batch(X_train, y_train, batch_size*num_gpus)\n",
    "test_data = get_batch(X_test, y_test, batch_size*num_gpus)\n",
    "rows, cols, channels = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.variable_scope(\"Inputs\") as scope:\n",
    "        X = tf.placeholder(tf.float32, [None, rows, cols, channels])\n",
    "        Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        \n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(lr,\n",
    "                                               global_step,\n",
    "                                               train_steps,\n",
    "                                               decay_rate,\n",
    "                                               staircase=True)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)       \n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)      \n",
    "\n",
    "    # Calculate the gradients for each model tower.\n",
    "    tower_grads = []\n",
    "    center_losses = []\n",
    "    softmax_losses = []\n",
    "    combined_losses = []\n",
    "    accuracies = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(\"/gpu:{}\".format(i)):\n",
    "            with tf.name_scope(\"resnet_{}\".format(i)) as scope:\n",
    "                _x = X[i * batch_size: (i+1) * batch_size]\n",
    "                _y = Y[i * batch_size: (i+1) * batch_size]\n",
    "\n",
    "                embeddings, pred_logits = classifier(_x, num_classes, num_embeddings, is_training)\n",
    "\n",
    "                center_loss, centers = get_center_loss(embeddings, tf.argmax(_y, axis=1), alpha, num_classes)\n",
    "                softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=_y, logits=pred_logits))\n",
    "                combined_loss = softmax_loss + ratio * center_loss\n",
    "                grads = opt.compute_gradients(combined_loss) \n",
    "                \n",
    "                accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred_logits, 1), tf.argmax(_y, 1)), tf.float32))\n",
    "\n",
    "                center_losses.append(center_loss)\n",
    "                softmax_losses.append(softmax_loss)\n",
    "                combined_losses.append(combined_loss)\n",
    "                accuracies.append(accuracy)\n",
    "                tower_grads.append(grads)\n",
    "\n",
    "    grads = average_gradients(tower_grads)    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step)\n",
    "        \n",
    "    avg_center_loss = tf.reduce_mean(center_losses)\n",
    "    avg_softmax_loss = tf.reduce_mean(softmax_losses)\n",
    "    avg_combined_loss = tf.reduce_mean(combined_losses)\n",
    "    avg_accuracies = tf.reduce_mean(accuracies)\n",
    "\n",
    "    tf.summary.scalar(\"avg_center_loss\", avg_center_loss)      \n",
    "    tf.summary.scalar(\"avg_softmax_loss\", avg_softmax_loss)      \n",
    "    tf.summary.scalar(\"avg_combined_loss\", avg_combined_loss) \n",
    "    tf.summary.scalar(\"avg_accuracies\", avg_accuracies) \n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver(max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement = True)) as sess:\n",
    "    train_writer = tf.summary.FileWriter(tensorboard_path+\"train\", sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(tensorboard_path+\"test\", sess.graph)\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step in range(1, train_steps + 1):\n",
    "            batch_x, batch_y = next(train_data)\n",
    "            _, summary, curr_step = sess.run([apply_gradient_op, merged, global_step], feed_dict={X: batch_x, Y: batch_y, is_training: True})\n",
    "            train_writer.add_summary(summary, curr_step)\n",
    "        \n",
    "        for step in range(1, test_steps+1):\n",
    "            batch_x, batch_y = next(test_data)\n",
    "            summary = sess.run(merged, feed_dict={X: batch_x, Y: batch_y, is_training: False})\n",
    "            test_writer.add_summary(summary, (epoch*test_steps)+step)\n",
    "            \n",
    "        if (epoch % save_step == 0):\n",
    "            saver.save(sess, weights_path, global_step=curr_step)\n",
    "            \n",
    "        sys.stdout.write(\"Current Epoch: {}\\r\".format(epoch))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "    saver.save(sess, weights_path, global_step=curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, iters=1000, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Fits a 2-parameter Weibull distribution to the given data using maximum-likelihood estimation.\n",
    "    :param x: 1d-ndarray of samples from an (unknown) distribution. Each value must satisfy x > 0.\n",
    "    :param iters: Maximum number of iterations\n",
    "    :param eps: Stopping criterion. Fit is stopped ff the change within two iterations is smaller than eps.\n",
    "    :return: Tuple (Shape, Scale) which can be (NaN, NaN) if a fit is impossible.\n",
    "        Impossible fits may be due to 0-values in x.\n",
    "    \"\"\"\n",
    "    # fit k via MLE\n",
    "    ln_x = np.log(x+eps)\n",
    "    k = 1.\n",
    "    k_t_1 = k\n",
    "\n",
    "    for t in range(iters):\n",
    "        x_k = x ** k\n",
    "        x_k_ln_x = x_k * ln_x\n",
    "        ff = np.sum(x_k_ln_x)\n",
    "        fg = np.sum(x_k)\n",
    "        f = ff / fg - np.mean(ln_x) - (1. / k)\n",
    "\n",
    "        # Calculate second derivative d^2f/dk^2\n",
    "        ff_prime = np.sum(x_k_ln_x * ln_x)\n",
    "        fg_prime = ff\n",
    "        f_prime = (ff_prime/fg - (ff/fg * fg_prime/fg)) + (1. / (k*k))\n",
    "\n",
    "        # Newton-Raphson method k = k - f(k;x)/f'(k;x)\n",
    "        k -= f/f_prime\n",
    "\n",
    "        if np.isnan(f):\n",
    "            return np.nan, np.nan\n",
    "        if abs(k - k_t_1) < eps:\n",
    "            break\n",
    "\n",
    "        k_t_1 = k\n",
    "\n",
    "    lam = np.mean(x ** k) ** (1.0 / k)\n",
    "\n",
    "    return k, lam\n",
    "\n",
    "\n",
    "def psi_i_dist(dist, k_i, lambda_i):\n",
    "    \"\"\"\n",
    "    Gives the probability of sample inclusion\n",
    "    :param dist: Numpy vector of distances between samples\n",
    "    :param lambda_i: Scale of the Weibull fitting\n",
    "    :param k_i: Shape of the Weibull fitting\n",
    "    :return: PSI = Probability of Sample Inclusion. This is the probability that x' is included in the boundary estimated by x_i\n",
    "    \"\"\"\n",
    "    return np.exp(-(((np.abs(dist))/lambda_i)**k_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "X_evm_train = []\n",
    "X_evm_test = []\n",
    "X_evm_data = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"/home/kjakkala/neuralwave/data/weights/central_loss/\"))\n",
    "    \n",
    "    for step in range(X_test.shape[0]):\n",
    "        X_evm_test.append(sess.run([embeddings], feed_dict={X: np.expand_dims(X_test[step], axis=0), is_training: False}))\n",
    "        \n",
    "    for step in range(X_train.shape[0]):\n",
    "        X_evm_train.append(sess.run([embeddings], feed_dict={X: np.expand_dims(X_train[step], axis=0), is_training: False}))\n",
    "\n",
    "    for step in range(X_data.shape[0]):\n",
    "        X_evm_data.append(sess.run([embeddings], feed_dict={X: np.expand_dims(X_data[step], axis=0), is_training: False}))\n",
    "\n",
    "    centers_final = sess.run([centers])\n",
    "\n",
    "X_evm_test = np.squeeze(X_evm_test)\n",
    "X_evm_train = np.squeeze(X_evm_train)\n",
    "X_evm_data = np.squeeze(X_evm_data)\n",
    "centers_final = np.squeeze(centers_final)\n",
    "print(X_evm_test.shape, X_evm_train.shape, X_evm_data.shape, centers_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tow = 50\n",
    "weibull_models = []\n",
    "for i in range(num_classes):\n",
    "    train_ind = np.squeeze(np.where(y_train_n == i))\n",
    "    D = pairwise_distances(X_evm_train[train_ind], X_evm_train, metric=\"euclidean\", n_jobs=1)\n",
    "    for j in range(D.shape[0]):\n",
    "        D_tmp = np.sort(D[j])\n",
    "        D_tmp = D_tmp[np.where(D_tmp>0)][:tow]\n",
    "        weibull_models.append(fit(D_tmp, iters=100, eps=1e-6))\n",
    "weibull_models = np.array(weibull_models)\n",
    "weibull_models.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evm(data):\n",
    "    D = pairwise_distances(X_evm_train, data, metric=\"euclidean\", n_jobs=1)\n",
    "    preds = np.zeros_like(D)\n",
    "    for i in range(X_evm_train.shape[0]):\n",
    "        for j in range(data.shape[0]):\n",
    "            preds[i, j] = psi_i_dist(D[i, j], weibull_models[i][0], weibull_models[i][1])\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evm = get_evm(X_evm_test)\n",
    "np.mean(np.equal(y_train_n[np.argmax(evm, axis=0)], y_test_n))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_preds(data):\n",
    "    class_thresholds = []\n",
    "    for i in range(num_classes):\n",
    "        class_thresholds.append(np.max(np.min(pairwise_distances(X_evm_train[np.where(y_train_n == i)], centers_final), axis=1)))\n",
    "    class_thresholds = np.array(class_thresholds)\n",
    "    class_thresholds += class_thresholds*2.4\n",
    "    \n",
    "    distance = np.min(pairwise_distances(data, centers_final), axis=1)\n",
    "    class_preds = np.argmin(pairwise_distances(data, centers_final), axis=1)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        if (distance[i] > class_thresholds[class_preds[i]]):\n",
    "            class_preds[i] = -1\n",
    "            \n",
    "    return class_preds, distance\n",
    "\n",
    "print(np.mean(np.equal(get_preds(X_evm_train)[0], y_train_n))*100)\n",
    "print(np.mean(np.equal(get_preds(X_evm_test)[0], y_test_n))*100)\n",
    "print(np.mean(np.equal(get_preds(X_evm_data)[0], y_data))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
