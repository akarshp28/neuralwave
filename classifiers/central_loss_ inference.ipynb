{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block, is_training):\n",
    "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters[0], kernel_size[0], padding=\"same\")\n",
    "\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters[1], kernel_size[1], padding=\"same\")\n",
    "\n",
    "        x = tf.concat([x, input_tensor], axis=-1)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, is_training, strides=2):\n",
    "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters[0], kernel_size[0], strides=strides, padding=\"same\")\n",
    "\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters[1], kernel_size[1], padding=\"same\")\n",
    "\n",
    "        shortcut = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
    "        shortcut = tf.nn.relu(shortcut)\n",
    "        shortcut = tf.layers.conv2d(shortcut, filters[1], kernel_size[1], strides=strides, padding=\"same\")\n",
    "\n",
    "        x = tf.concat([x, shortcut], axis=-1)\n",
    "    return x\n",
    "\n",
    "def classifier(inputs, num_classes, num_embeddings, is_training):\n",
    "    with tf.variable_scope(\"stage-1_block-a\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(inputs, 64, 7, strides=2, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.max_pooling2d(x, 3, strides=2, padding=\"same\")\n",
    "\n",
    "    x = conv_block    (x, [1, 3], [64, 256], stage=2, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [1, 3], [64, 256], stage=2, block='b', is_training=is_training)\n",
    "\n",
    "    x = conv_block    (x, [1, 3], [128, 512], stage=3, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [1, 3], [128, 512], stage=3, block='b', is_training=is_training)\n",
    "\n",
    "    x = conv_block    (x, [1, 3], [256, 1024], stage=4, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [1, 3], [256, 1024], stage=4, block='b', is_training=is_training)\n",
    "\n",
    "    x = conv_block    (x, [1, 3], [512, 2048], stage=5, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [1, 3], [512, 2048], stage=5, block='b', is_training=is_training)\n",
    "\n",
    "    with tf.variable_scope(\"embeddings\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.average_pooling2d(x, (x.get_shape()[-3], x.get_shape()[-2]), 1) #global average pooling\n",
    "        x = tf.layers.flatten(x)\n",
    "        embeddings = tf.layers.dense(x, num_embeddings, activation=None)\n",
    "        \n",
    "    with tf.variable_scope(\"logits\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        pred_logits = tf.nn.relu(embeddings)\n",
    "        pred_logits = tf.layers.dense(pred_logits, num_classes, activation=None)\n",
    "\n",
    "    return embeddings, pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_loss(features, labels, alpha, num_classes):\n",
    "    \n",
    "    len_features = features.get_shape()[1] \n",
    "    with tf.variable_scope(\"central_loss\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        centers = tf.get_variable('centers', [num_classes, len_features], dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "    labels = tf.reshape(labels, [-1])\n",
    "\n",
    "    centers_batch = tf.gather(centers, labels)\n",
    "    loss = tf.nn.l2_loss(features - centers_batch)\n",
    "\n",
    "    unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
    "    appear_times = tf.gather(unique_count, unique_idx)\n",
    "    appear_times = tf.reshape(appear_times, [-1, 1])\n",
    "\n",
    "    diff = centers_batch - features\n",
    "    diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
    "    diff = alpha * diff\n",
    "\n",
    "    centers_update_op = tf.scatter_sub(centers, labels, diff)\n",
    "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, centers_update_op)\n",
    "\n",
    "    return loss, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p], b[p]\n",
    "\n",
    "def get_batch(data_x, data_y, batch_size):\n",
    "    while True:\n",
    "        data_x, data_y = unison_shuffled_copies(data_x, data_y)\n",
    "        for index in range(0, data_x.shape[0], batch_size):\n",
    "            x, y = data_x[index:index+batch_size], data_y[index:index+batch_size]\n",
    "            if x.shape[0] == batch_size:\n",
    "                yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1631, 500, 135, 1) (1631, 48) (288, 500, 135, 1) (288, 48) (179, 500, 135, 1) (179,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = '/home/kjakkala/neuralwave/data/CSI_preprocessed_48_pdf_15.h5'\n",
    "intruder_dir=\"/home/kjakkala/neuralwave/data/CSI_INTRUDER_48_pdf_15.h5\"\n",
    "weights_path =  \"/home/kjakkala/neuralwave/data/weights/central_loss_48_pdf_15_400\"\n",
    "\n",
    "num_embeddings = 64\n",
    "num_classes = 48\n",
    "save_step = 10\n",
    "decay_rate = 0.995\n",
    "batch_size = 32\n",
    "num_gpus = 4\n",
    "alpha = 0.5\n",
    "ratio = 0.8\n",
    "epochs = 400\n",
    "lr = 1e-4\n",
    "\n",
    "hf = h5py.File(data_file, 'r')\n",
    "X_train = np.expand_dims(hf.get('X_train'), axis=-1)[:, :, ::2]\n",
    "X_test = np.expand_dims(hf.get('X_test'), axis=-1)[:, :, ::2]\n",
    "y_train = np.eye(num_classes)[hf.get('y_train')]\n",
    "y_test = np.eye(num_classes)[hf.get('y_test')]\n",
    "y_train_n = np.array(hf.get('y_train'))\n",
    "y_test_n = np.array(hf.get('y_test'))\n",
    "train_classes = np.array(hf.get('labels')).astype(str)\n",
    "hf.close()\n",
    "\n",
    "hf = h5py.File(intruder_dir, 'r')\n",
    "X_data = np.expand_dims(hf.get('X_data'), axis=-1)[:, :, ::2]\n",
    "y_data = np.array(hf.get('y_data'))\n",
    "data_classes = np.array(hf.get('labels')).astype(str)\n",
    "hf.close() \n",
    "\n",
    "rows, cols, channels = X_train.shape[1:]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_data.shape, y_data.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.variable_scope(\"Inputs\") as scope:\n",
    "        X = tf.placeholder(tf.float32, [None, rows, cols, channels])\n",
    "        Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        \n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # Calculate the gradients for each model tower.\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        with tf.name_scope(\"resnet_0\") as scope:\n",
    "            embeddings, pred_logits = classifier(X, num_classes, num_embeddings, is_training)\n",
    "\n",
    "            center_loss, centers = get_center_loss(embeddings, tf.argmax(Y, axis=1), alpha, num_classes)\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver(max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/kjakkala/neuralwave/data/weights/central_loss_48_pdf_15_400/central_loss_model.ckpt-4800\n",
      "(179, 48) (288, 48) (1631, 48) (179, 64) (288, 64) (1631, 64) (48, 64)\n"
     ]
    }
   ],
   "source": [
    "X_data_embeddings = []\n",
    "X_test_embeddings = []\n",
    "X_data_logits = []\n",
    "X_test_logits = []\n",
    "X_train_embeddings = []\n",
    "X_train_logits = []\n",
    "final_centers = []\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement = True)) as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(weights_path))\n",
    "    \n",
    "    final_centers = sess.run([centers], feed_dict={is_training: False})\n",
    "\n",
    "    for step in range(X_data.shape[0]):\n",
    "        tmp1, tmp2 = sess.run([embeddings, pred_logits], feed_dict={X: np.expand_dims(X_data[step], axis=0), is_training: False})\n",
    "        X_data_embeddings.append(tmp1)\n",
    "        X_data_logits.append(tmp2)\n",
    "\n",
    "    for step in range(X_test.shape[0]):\n",
    "        tmp1, tmp2 = sess.run([embeddings, pred_logits], feed_dict={X: np.expand_dims(X_test[step], axis=0), is_training: False})\n",
    "        X_test_embeddings.append(tmp1)\n",
    "        X_test_logits.append(tmp2)\n",
    "        \n",
    "    for step in range(X_train.shape[0]):\n",
    "        tmp1, tmp2 = sess.run([embeddings, pred_logits], feed_dict={X: np.expand_dims(X_train[step], axis=0), is_training: False})\n",
    "        X_train_embeddings.append(tmp1)\n",
    "        X_train_logits.append(tmp2)\n",
    "\n",
    "X_test_embeddings = np.squeeze(X_test_embeddings)\n",
    "X_data_embeddings = np.squeeze(X_data_embeddings)\n",
    "X_train_embeddings = np.squeeze(X_train_embeddings)\n",
    "X_train_logits = np.squeeze(X_train_logits)\n",
    "X_test_logits = np.squeeze(X_test_logits)\n",
    "X_data_logits = np.squeeze(X_data_logits)\n",
    "final_centers = np.squeeze(final_centers)\n",
    "\n",
    "print(X_data_logits.shape, X_test_logits.shape, X_train_logits.shape, X_data_embeddings.shape, X_test_embeddings.shape, X_train_embeddings.shape, final_centers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9861111111111112\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.equal(np.argmax(X_test_logits, axis=-1), np.argmax(y_test, axis=-1))))\n",
    "print(np.mean(np.equal(np.argmax(X_data_logits, axis=-1)[20:40], np.ones(20)*8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "champ           \tchamp_conf                    \n",
      "jingwei         \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "jingwei         \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "champ           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "siddu           \tchamp_conf                    \n",
      "echo            \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "echo            \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original                \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "benjamin_poole  \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "jingwei         \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "huijie_chen     \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "jingwei         \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_original_1              \n",
      "champ           \tchamp_random                  \n",
      "jingwei         \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "jingwei         \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "jingwei         \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "champ           \tchamp_random                  \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "siddu           \tkalvik_conf                   \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "jingwei         \tkalvik_original_1             \n",
      "champ           \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "champ           \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "champ           \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "echo            \tkalvik_original_1             \n",
      "prabhu          \tkalvik_random                 \n",
      "kalvik          \tkalvik_random                 \n",
      "champ           \tkalvik_random                 \n",
      "kalvik          \tkalvik_random                 \n",
      "kalvik          \tkalvik_random                 \n",
      "kalvik          \tkalvik_random                 \n",
      "prabhu          \tkalvik_random                 \n",
      "kalvik          \tkalvik_random                 \n",
      "kalvik          \tkalvik_random                 \n",
      "kalvik          \tkalvik_random                 \n",
      "champ           \tkalvik_random                 \n",
      "champ           \tkalvik_random                 \n",
      "champ           \tkalvik_random                 \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "siddu           \tprabhu_conf                   \n",
      "echo            \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "echo            \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "echo            \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "echo            \tprabhu_original_1             \n",
      "echo            \tprabhu_original_1             \n",
      "echo            \tprabhu_original_1             \n",
      "huijie_chen     \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "echo            \tprabhu_original_1             \n",
      "echo            \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "echo            \tprabhu_original_1             \n",
      "champ           \tprabhu_original_1             \n",
      "prabhu          \tprabhu_random                 \n",
      "champ           \tprabhu_random                 \n",
      "prabhu          \tprabhu_random                 \n",
      "prabhu          \tprabhu_random                 \n",
      "prabhu          \tprabhu_random                 \n",
      "prabhu          \tprabhu_random                 \n",
      "champ           \tprabhu_random                 \n",
      "champ           \tprabhu_random                 \n",
      "prabhu          \tprabhu_random                 \n",
      "prabhu          \tprabhu_random                 \n",
      "prabhu          \tprabhu_random                 \n",
      "champ           \tprabhu_random                 \n",
      "prabhu          \tprabhu_random                 \n"
     ]
    }
   ],
   "source": [
    "pred_y = np.argmax(X_data_logits, axis=-1)\n",
    "for i in range(len(pred_y)):\n",
    "    print(\"{: <16}\\t{: <30}\".format(train_classes[pred_y[i]], data_classes[y_data[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(data):\n",
    "    class_thresholds = []\n",
    "    for i in range(num_classes):\n",
    "        class_thresholds.append(np.max(np.min(pairwise_distances(X_train_embeddings[np.where(y_train_n == i)], final_centers), axis=1)))\n",
    "    class_thresholds = np.array(class_thresholds)\n",
    "    class_thresholds += class_thresholds*3.1\n",
    "    \n",
    "    distance = np.min(pairwise_distances(data, final_centers), axis=1)\n",
    "    class_preds = np.argmin(pairwise_distances(data, final_centers), axis=1)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        if (distance[i] > class_thresholds[class_preds[i]]):\n",
    "            class_preds[i] = -1\n",
    "            \n",
    "    return class_preds, distance\n",
    "\n",
    "print(np.mean(np.equal(get_preds(X_train_embeddings)[0], y_train_n))*100)\n",
    "print(np.mean(np.equal(get_preds(X_test_embeddings)[0], y_test_n))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intruders = get_preds(X_data_embeddings)[0]\n",
    "for i in range(len(intruders)):\n",
    "    print(\"{: <5}\\t{: <30}\".format(train_classes[intruders[i]], data_classes[y_data[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
