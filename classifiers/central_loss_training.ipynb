{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, filters, stage, block, is_training):\n",
    "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(input_tensor, filters[0], 1, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.layers.conv2d(x, filters[0], 3, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters[1], 1, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "\n",
    "        x = tf.math.add(x, input_tensor)\n",
    "        x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, filters, stage, block, is_training, strides=2):\n",
    "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(input_tensor, filters[0], 1, strides=strides, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.layers.conv2d(x, filters[0], 3, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters[1], 1, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "\n",
    "        shortcut = tf.layers.conv2d(input_tensor, filters[1], 1, strides=strides, padding=\"same\")\n",
    "        shortcut = tf.layers.batch_normalization(shortcut, training=is_training)\n",
    "\n",
    "        x = tf.math.add(x, shortcut)\n",
    "        x = tf.nn.relu(x)        \n",
    "    return x\n",
    "\n",
    "def classifier(inputs, num_classes, num_embeddings, is_training):\n",
    "    with tf.variable_scope(\"stage-1_block-a\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(inputs, 16, 7, padding=\"same\")\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.max_pooling2d(x, 3, strides=2, padding=\"same\")\n",
    "\n",
    "    x = conv_block    (x, [16, 64], stage=2, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [16, 64], stage=2, block='b', is_training=is_training)\n",
    "\n",
    "    x = conv_block    (x, [32, 128], stage=3, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [32, 128], stage=3, block='b', is_training=is_training)\n",
    "\n",
    "    x = conv_block    (x, [64, 128], stage=4, block='a', is_training=is_training)\n",
    "    x = identity_block(x, [64, 128], stage=4, block='b', is_training=is_training)\n",
    "\n",
    "    with tf.variable_scope(\"embeddings\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.average_pooling2d(x, (x.get_shape()[-3], x.get_shape()[-2]), 1) #global average pooling\n",
    "        x = tf.layers.flatten(x)\n",
    "        embeddings = tf.layers.dense(x, num_embeddings, activation=None)\n",
    "        \n",
    "    with tf.variable_scope(\"logits\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        pred_logits = tf.nn.relu(embeddings)\n",
    "        pred_logits = tf.layers.dense(pred_logits, num_classes, activation=None)\n",
    "\n",
    "    return embeddings, pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_loss(features, labels, alpha, num_classes):\n",
    "    \n",
    "    len_features = features.get_shape()[1] \n",
    "    with tf.variable_scope(\"central_loss\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        centers = tf.get_variable('centers', [num_classes, len_features], dtype=tf.float32,\n",
    "            initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "    labels = tf.reshape(labels, [-1])\n",
    "\n",
    "    centers_batch = tf.gather(centers, labels)\n",
    "    loss = tf.nn.l2_loss(features - centers_batch)\n",
    "\n",
    "    unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
    "    appear_times = tf.gather(unique_count, unique_idx)\n",
    "    appear_times = tf.reshape(appear_times, [-1, 1])\n",
    "\n",
    "    diff = centers_batch - features\n",
    "    diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
    "    diff = alpha * diff\n",
    "\n",
    "    centers_update_op = tf.scatter_sub(centers, labels, diff)\n",
    "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, centers_update_op)\n",
    "\n",
    "    return loss, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p], b[p]\n",
    "\n",
    "def get_batch(data_x, data_y, batch_size):\n",
    "    while True:\n",
    "        data_x, data_y = unison_shuffled_copies(data_x, data_y)\n",
    "        for index in range(0, data_x.shape[0], batch_size):\n",
    "            x, y = data_x[index:index+batch_size], data_y[index:index+batch_size]\n",
    "            if x.shape[0] == batch_size:\n",
    "                yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360, 2000, 270, 1) (1360, 40) (240, 2000, 270, 1) (240, 40)\n"
     ]
    }
   ],
   "source": [
    "data_file =        \"/home/kjakkala/neuralwave/data/CSI_50_500.h5\"\n",
    "tensorboard_path = \"/home/kjakkala/neuralwave/data/logs/central_loss_50_500/\"\n",
    "weights_path =     \"/home/kjakkala/neuralwave/data/weights/central_loss_50_500/central_loss_model.ckpt\"\n",
    "\n",
    "num_embeddings = 64\n",
    "save_step = 25\n",
    "decay_rate = 0.999\n",
    "batch_size = 16\n",
    "num_gpus = 4\n",
    "alpha = 0.5\n",
    "ratio = 0.9\n",
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "\n",
    "hf = h5py.File(data_file, 'r')\n",
    "train_classes = np.array(hf.get('labels')).astype(str)\n",
    "num_classes = len(train_classes)\n",
    "X_train = np.expand_dims(hf.get('X_train'), axis=-1)\n",
    "X_test = np.expand_dims(hf.get('X_test'), axis=-1)\n",
    "y_train = np.eye(num_classes)[hf.get('y_train')]\n",
    "y_test = np.eye(num_classes)[hf.get('y_test')]\n",
    "hf.close()\n",
    "\n",
    "train_steps = X_train.shape[0]//(batch_size*num_gpus)\n",
    "test_steps = X_test.shape[0]//(batch_size*num_gpus)\n",
    "train_data = get_batch(X_train, y_train, batch_size*num_gpus)\n",
    "test_data = get_batch(X_test, y_test, batch_size*num_gpus)\n",
    "rows, cols, channels = X_train.shape[1:]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.variable_scope(\"Inputs\") as scope:\n",
    "        X = tf.placeholder(tf.float32, [None, rows, cols, channels])\n",
    "        Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        \n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(lr,\n",
    "                                               global_step,\n",
    "                                               train_steps,\n",
    "                                               decay_rate,\n",
    "                                               staircase=True)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)       \n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)      \n",
    "\n",
    "    # Calculate the gradients for each model tower.\n",
    "    tower_grads = []\n",
    "    center_losses = []\n",
    "    softmax_losses = []\n",
    "    combined_losses = []\n",
    "    accuracies = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(\"/gpu:{}\".format(i)):\n",
    "            with tf.name_scope(\"resnet_{}\".format(i)) as scope:\n",
    "                _x = X[i * batch_size: (i+1) * batch_size]\n",
    "                _y = Y[i * batch_size: (i+1) * batch_size]\n",
    "\n",
    "                embeddings, pred_logits = classifier(_x, num_classes, num_embeddings, is_training)\n",
    "\n",
    "                center_loss, centers = get_center_loss(embeddings, tf.argmax(_y, axis=1), alpha, num_classes)\n",
    "                softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=_y, logits=pred_logits))\n",
    "                combined_loss = softmax_loss + ratio * center_loss\n",
    "                grads = opt.compute_gradients(combined_loss) \n",
    "                \n",
    "                accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred_logits, 1), tf.argmax(_y, 1)), tf.float32))\n",
    "\n",
    "                center_losses.append(center_loss)\n",
    "                softmax_losses.append(softmax_loss)\n",
    "                combined_losses.append(combined_loss)\n",
    "                accuracies.append(accuracy)\n",
    "                tower_grads.append(grads)\n",
    "\n",
    "    grads = average_gradients(tower_grads)    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step)\n",
    "        \n",
    "    avg_center_loss = tf.reduce_mean(center_losses)\n",
    "    avg_softmax_loss = tf.reduce_mean(softmax_losses)\n",
    "    avg_combined_loss = tf.reduce_mean(combined_losses)\n",
    "    avg_accuracies = tf.reduce_mean(accuracies)\n",
    "\n",
    "    tf.summary.scalar(\"avg_center_loss\", avg_center_loss)      \n",
    "    tf.summary.scalar(\"avg_softmax_loss\", avg_softmax_loss)      \n",
    "    tf.summary.scalar(\"avg_combined_loss\", avg_combined_loss) \n",
    "    tf.summary.scalar(\"avg_accuracies\", avg_accuracies) \n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver(max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 78\r"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement = True)) as sess:\n",
    "    train_writer = tf.summary.FileWriter(tensorboard_path+\"train\", sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(tensorboard_path+\"test\", sess.graph)\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step in range(1, train_steps + 1):\n",
    "            batch_x, batch_y = next(train_data)\n",
    "            _, summary, curr_step = sess.run([apply_gradient_op, merged, global_step], feed_dict={X: batch_x, Y: batch_y, is_training: True})\n",
    "            train_writer.add_summary(summary, curr_step)\n",
    "        \n",
    "        for step in range(1, test_steps+1):\n",
    "            batch_x, batch_y = next(test_data)\n",
    "            summary = sess.run(merged, feed_dict={X: batch_x, Y: batch_y, is_training: False})\n",
    "            test_writer.add_summary(summary, (epoch*test_steps)+step)\n",
    "            \n",
    "        if (epoch % save_step == 0):\n",
    "            saver.save(sess, weights_path, global_step=curr_step)\n",
    "            \n",
    "        sys.stdout.write(\"Current Epoch: {}\\r\".format(epoch))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "    saver.save(sess, weights_path, global_step=curr_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
