{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, Conv1D, Flatten, Dropout, MaxPooling1D, Dense, BatchNormalization, Activation, UpSampling1D, Concatenate\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Lambda\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1107, 361, 1) (1107, 30)\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File('/scratck/kjakkala/neuralwave/data/pca_data.h5', 'r')\n",
    "X_train = np.expand_dims(hf.get('X_train'), axis=-1)\n",
    "X_test = np.expand_dims(hf.get('X_test'), axis=-1)\n",
    "y_train = np.eye(30)[hf.get('y_train')]\n",
    "y_test = np.eye(30)[hf.get('y_test')]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    filters1, filters2, filters3 = filters\n",
    "        \n",
    "    bn_axis = -1\n",
    "        \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv1D(filters1, 1,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters2, kernel_size,\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters3, 1,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=2):\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    bn_axis = -1\n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv1D(filters1, 1, strides=strides,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters2, kernel_size, padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters3, 1,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = layers.Conv1D(filters3, 1, strides=strides,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = layers.BatchNormalization(\n",
    "        axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMSoftmax(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, s, m, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        super(AMSoftmax, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[0][-1], self.output_dim),\n",
    "                                      initializer=TruncatedNormal(mean=0.0, stddev=1.0),\n",
    "                                      trainable=True)\n",
    "\n",
    "        super(AMSoftmax, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs): \n",
    "        x = inputs[0]\n",
    "        y = inputs[1]\n",
    "        kernel_norm = K.l2_normalize(self.kernel, 0)\n",
    "        cos_theta = K.dot(x, kernel_norm)\n",
    "        cos_theta = K.clip(cos_theta, -1,1) # for numerical steady\n",
    "        phi = cos_theta - self.m \n",
    "        adjust_theta = self.s * K.tf.where(K.tf.equal(y, 1), phi, cos_theta)\n",
    "        return K.softmax(adjust_theta)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 361, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv1D)         (None, 181, 8)       16          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 181, 8)       32          res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 181, 8)       0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv1D)         (None, 181, 8)       200         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 181, 8)       32          res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 181, 8)       0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv1D)         (None, 181, 16)      144         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv1D)          (None, 181, 16)      32          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 181, 16)      64          res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 181, 16)      64          res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 181, 16)      0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 181, 16)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv1D)         (None, 181, 8)       136         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 181, 8)       32          res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 181, 8)       0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv1D)         (None, 181, 8)       200         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 181, 8)       32          res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 181, 8)       0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv1D)         (None, 181, 16)      144         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 181, 16)      64          res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 181, 16)      0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 181, 16)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2896)         0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          370816      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "am_softmax_1 (AMSoftmax)        (None, 30)           3840        dense_1[0][0]                    \n",
      "                                                                 labels[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 375,848\n",
      "Trainable params: 375,688\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1107 samples, validate on 196 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 16.1179 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 3s - loss: 16.1179 - acc: 0.0000e+00 - val_loss: 16.1073 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 3s - loss: 16.1125 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 3s - loss: 16.0901 - acc: 0.0000e+00 - val_loss: 15.9759 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 3s - loss: 15.6646 - acc: 0.0108 - val_loss: 15.2140 - val_acc: 0.0357\n",
      "Epoch 6/100\n",
      " - 3s - loss: 14.9513 - acc: 0.0506 - val_loss: 15.0098 - val_acc: 0.0612\n",
      "Epoch 7/100\n",
      " - 3s - loss: 14.4813 - acc: 0.0885 - val_loss: 14.8020 - val_acc: 0.0765\n",
      "Epoch 8/100\n",
      " - 3s - loss: 14.1048 - acc: 0.1084 - val_loss: 14.6080 - val_acc: 0.0765\n",
      "Epoch 9/100\n",
      " - 3s - loss: 13.8507 - acc: 0.1220 - val_loss: 14.3400 - val_acc: 0.0765\n",
      "Epoch 10/100\n",
      " - 3s - loss: 13.5204 - acc: 0.1491 - val_loss: 14.2874 - val_acc: 0.0969\n",
      "Epoch 11/100\n",
      " - 3s - loss: 13.3001 - acc: 0.1707 - val_loss: 14.2099 - val_acc: 0.1071\n",
      "Epoch 12/100\n",
      " - 3s - loss: 13.1549 - acc: 0.1608 - val_loss: 13.9384 - val_acc: 0.1173\n",
      "Epoch 13/100\n",
      " - 3s - loss: 12.7940 - acc: 0.1969 - val_loss: 13.8404 - val_acc: 0.1224\n",
      "Epoch 14/100\n",
      " - 3s - loss: 12.7430 - acc: 0.2042 - val_loss: 13.9523 - val_acc: 0.1224\n",
      "Epoch 15/100\n",
      " - 3s - loss: 12.5409 - acc: 0.2105 - val_loss: 13.3088 - val_acc: 0.1378\n",
      "Epoch 16/100\n",
      " - 3s - loss: 12.0333 - acc: 0.2439 - val_loss: 12.9216 - val_acc: 0.1837\n",
      "Epoch 17/100\n",
      " - 3s - loss: 11.9451 - acc: 0.2575 - val_loss: 13.0842 - val_acc: 0.1684\n",
      "Epoch 18/100\n",
      " - 3s - loss: 11.8927 - acc: 0.2575 - val_loss: 13.2603 - val_acc: 0.1531\n",
      "Epoch 19/100\n",
      " - 3s - loss: 11.8596 - acc: 0.2611 - val_loss: 12.8369 - val_acc: 0.1735\n",
      "Epoch 20/100\n",
      " - 3s - loss: 11.8778 - acc: 0.2529 - val_loss: 12.8402 - val_acc: 0.1735\n",
      "Epoch 21/100\n",
      " - 3s - loss: 11.7142 - acc: 0.2674 - val_loss: 12.7885 - val_acc: 0.1837\n",
      "Epoch 22/100\n",
      " - 3s - loss: 11.6479 - acc: 0.2746 - val_loss: 12.9659 - val_acc: 0.1684\n",
      "Epoch 23/100\n",
      " - 3s - loss: 11.6158 - acc: 0.2782 - val_loss: 12.9163 - val_acc: 0.1735\n",
      "Epoch 24/100\n",
      " - 3s - loss: 11.6106 - acc: 0.2782 - val_loss: 12.9804 - val_acc: 0.1786\n",
      "Epoch 25/100\n",
      " - 3s - loss: 11.6109 - acc: 0.2791 - val_loss: 12.8042 - val_acc: 0.1837\n",
      "Epoch 26/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7984 - val_acc: 0.1837\n",
      "Epoch 27/100\n",
      " - 3s - loss: 11.6046 - acc: 0.2800 - val_loss: 12.7982 - val_acc: 0.1837\n",
      "Epoch 28/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7977 - val_acc: 0.1837\n",
      "Epoch 29/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7970 - val_acc: 0.1837\n",
      "Epoch 30/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7975 - val_acc: 0.1837\n",
      "Epoch 31/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7969 - val_acc: 0.1837\n",
      "Epoch 32/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7966 - val_acc: 0.1837\n",
      "Epoch 33/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7959 - val_acc: 0.1837\n",
      "Epoch 34/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7955 - val_acc: 0.1837\n",
      "Epoch 35/100\n",
      " - 3s - loss: 11.6048 - acc: 0.2800 - val_loss: 12.8176 - val_acc: 0.1837\n",
      "Epoch 36/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.8197 - val_acc: 0.1837\n",
      "Epoch 37/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.8182 - val_acc: 0.1837\n",
      "Epoch 38/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.8182 - val_acc: 0.1786\n",
      "Epoch 39/100\n",
      " - 3s - loss: 11.6048 - acc: 0.2800 - val_loss: 12.8116 - val_acc: 0.1786\n",
      "Epoch 40/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7562 - val_acc: 0.1837\n",
      "Epoch 41/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7518 - val_acc: 0.1837\n",
      "Epoch 42/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7501 - val_acc: 0.1837\n",
      "Epoch 43/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7504 - val_acc: 0.1837\n",
      "Epoch 44/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7532 - val_acc: 0.1837\n",
      "Epoch 45/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7528 - val_acc: 0.1837\n",
      "Epoch 46/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7554 - val_acc: 0.1837\n",
      "Epoch 47/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7553 - val_acc: 0.1837\n",
      "Epoch 48/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7551 - val_acc: 0.1837\n",
      "Epoch 49/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7552 - val_acc: 0.1837\n",
      "Epoch 50/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7554 - val_acc: 0.1837\n",
      "Epoch 51/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7559 - val_acc: 0.1837\n",
      "Epoch 52/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7555 - val_acc: 0.1837\n",
      "Epoch 53/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7558 - val_acc: 0.1837\n",
      "Epoch 54/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7548 - val_acc: 0.1837\n",
      "Epoch 55/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7534 - val_acc: 0.1837\n",
      "Epoch 56/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7537 - val_acc: 0.1837\n",
      "Epoch 57/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7551 - val_acc: 0.1837\n",
      "Epoch 58/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7544 - val_acc: 0.1837\n",
      "Epoch 59/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7536 - val_acc: 0.1837\n",
      "Epoch 60/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7543 - val_acc: 0.1837\n",
      "Epoch 61/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7551 - val_acc: 0.1837\n",
      "Epoch 62/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7548 - val_acc: 0.1837\n",
      "Epoch 63/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7561 - val_acc: 0.1837\n",
      "Epoch 64/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7560 - val_acc: 0.1837\n",
      "Epoch 65/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7558 - val_acc: 0.1837\n",
      "Epoch 66/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7549 - val_acc: 0.1837\n",
      "Epoch 67/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7561 - val_acc: 0.1837\n",
      "Epoch 68/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7555 - val_acc: 0.1837\n",
      "Epoch 69/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7564 - val_acc: 0.1837\n",
      "Epoch 70/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7560 - val_acc: 0.1837\n",
      "Epoch 71/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7542 - val_acc: 0.1837\n",
      "Epoch 72/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7531 - val_acc: 0.1837\n",
      "Epoch 73/100\n",
      " - 3s - loss: 11.6047 - acc: 0.2800 - val_loss: 12.7459 - val_acc: 0.1837\n",
      "Epoch 74/100\n",
      " - 3s - loss: 11.6047 - acc: 0.2800 - val_loss: 12.7422 - val_acc: 0.1939\n",
      "Epoch 75/100\n",
      " - 3s - loss: 11.6045 - acc: 0.2800 - val_loss: 12.7346 - val_acc: 0.1939\n",
      "Epoch 76/100\n",
      " - 3s - loss: 11.6044 - acc: 0.2800 - val_loss: 12.7326 - val_acc: 0.1888\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-40380e8991ce>\", line 17, in <module>\n",
      "    model.fit(x=[X_train, y_train], y=y_train, epochs=100, validation_data=([X_test, y_test], y_test), verbose=2)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1037, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2658, in __call__\n",
      "    if hasattr(get_session(), '_make_callable_from_options'):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 207, in get_session\n",
      "    if not hasattr(session, 'list_devices'):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(X_train.shape[-2], 1), name='input')\n",
    "labels = Input(shape=(30,), name='labels')\n",
    "\n",
    "x = conv_block(inputs, 3, [4, 4, 8], stage=1, block='a', strides=2)\n",
    "x = identity_block(x, 3, [4, 4, 8], stage=1, block='b')\n",
    "\n",
    "x = conv_block(inputs, 3, [8, 8, 16], stage=2, block='a', strides=2)\n",
    "x = identity_block(x, 3, [8, 8, 16], stage=2, block='b')\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = AMSoftmax(30, s=30, m=0.55)([x, labels])\n",
    "\n",
    "model = Model(inputs=[inputs, labels], outputs=x)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=1e-4), metrics=['acc'])\n",
    "model.fit(x=[X_train, y_train], y=y_train, epochs=100, validation_data=([X_test, y_test], y_test), verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
